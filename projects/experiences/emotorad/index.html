<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Projects</title>
    <link rel="icon" type="image/x-icon" href="/images/favicon.png" />
    <link rel="stylesheet" href="../../../index.css" />
    <link rel="stylesheet" href="../../project-styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Epilogue:wght@400;500;700&display=swap"
      rel="stylesheet"
    />
    <script src="../../project-content.js"></script>
  </head>
  <body>
    <div class="navbar">
      <a href="../../..">Home</a>
      <a href="../.." class="active">Projects</a>
      <a href="../../../about">About</a>
      <a href="../../../contact">Contact</a>
    </div>

    <div class="content">
      <div class="wrapper">
        <div class="left-sidebar">
          <h1>Contents</h1>
        </div>

        <div class="project-content">
          <h1 class="section-heading">Overview</h1>
          <h2 class="section-subheading">Company Name</h2>
          <p>Emotorad</p>

          <h2 class="section-subheading">Duration</h2>
          <p>August 2023 - March 2024</p>

          <h2 class="section-subheading">Role</h2>
          <p>Cloud and Backend Intern and Consultant</p>

          <h2 class="section-subheading">TLDR</h2>
          <p>
            At Emotorad, I led the design and development of a scalable backend
            architecture for real-time electric bike data ingestion, utilizing
            MQTT brokers and stateful Apache Flink applications for data
            aggregation and processing with Cassandra, Redis, and Kafka as
            sinks. I developed and maintained Go REST microservices for
            dashboards and the companion app, orchestrated deployment using AWS
            and Kubernetes, and managed a data lake on AWS S3 with queries via
            Presto. Additionally, I deployed Firebase cloud functions for push
            notifications and contributed to team growth by interviewing
            candidates for full-time and intern roles.
          </p>

          <h1 class="section-heading">Architecture & Design</h1>
          <h2 class="section-subheading">Overview</h2>
          <div class="image-container">
            <a
              href="http://papaya147.github.io/image-viewer?img=https://papaya147.github.io/projects/experiences/emotorad/rendered/overview.svg"
            >
              <img src="rendered/overview.svg" alt="Emotorad Architecture" />
            </a>
          </div>
          <p>
            The problem statement was simple. Ingest all the data from each
            bicycle that I possibly could and store it for later usage. Usage
            for the data was given in additional feature requests, but that was
            simple enough once I was able to collect and store the data
            effectively.
          </p>
          <p>
            This was my first time working on something that could potentially
            scale to tens of thousands of users, so I had to do it right.
            Luckily, I had some help from the backend team at Ola Electric to
            give me pointers along the way.
          </p>
          <p>
            There are quite a few components at work here, let's break it down.
          </p>
          <h2 class="section-subheading">Why I Ditched WebSockets for MQTT</h2>
          <p>
            So initially, everyone was like "let's just use WebSockets for the
            bike data" - which honestly makes sense on the surface since it's
            real-time communication, right? But here's the thing: WebSockets are
            a pain when you're dealing with thousands of IoT devices. Every
            single bike would need to keep a constant connection open to my
            servers, which is like having 10,000+ people all trying to have
            individual phone conversations with you at the same time. The real
            kicker is that WebSockets are stateful, meaning if one server goes
            down, all the bikes connected to it lose their connection and have
            to reconnect. Plus, scaling horizontally means figuring out how to
            share all that connection state between servers - it's doable, but
            it's a headache you don't want to deal with.
          </p>
          <p>
            MQTT is basically the gold standard for IoT stuff, and for good
            reason. It's way more lightweight - instead of each bike maintaining
            a heavy connection, they just send messages to a broker that handles
            all the routing. It's like switching from everyone calling you
            directly to having a really efficient receptionist who takes
            messages and passes them along. I went with EMQX because they had
            this super clean Docker setup that I could get running in like 10
            minutes, and their pricing was actually reasonable if I needed to
            upgrade to their enterprise features later. Sometimes the simplest
            solution that gets you moving quickly is the right one.
          </p>
          <h2 class="section-subheading">
            Kafka - Handling the Message Firehose
          </h2>
          <p>
            Honestly, Kafka was kind of a no-brainer once I saw the scale I was
            dealing with. When you've got 10,000+ bikes constantly streaming
            telemetry data, you need something that can handle that firehose of
            messages without breaking a sweat. The beautiful thing about Kafka
            is that it doesn't just ingest all that incoming data; it also lets
            multiple systems tap into that same stream simultaneously. So while
            my real-time dashboard is pulling the latest GPS coordinates to show
            where bikes are right now, my analytics pipeline is also reading
            that same data to build things like route heat maps and battery
            usage patterns. I went with AWS MSK because managing your own Kafka
            cluster is a headache you don't want to deal with if you can avoid
            it - AWS handles all the infrastructure stuff so I could focus on
            actually building features.
          </p>
          <p>
            Now, here's where things got interesting with the processing side. I
            initially thought I'd use AWS's other streaming services like
            Kinesis and Data Firehose, but they fell short when it came to
            stateful operations. Building real-time route maps isn't just about
            processing individual messages - you need to remember state across
            multiple MQTT messages from the same bike to piece together a
            coherent route. AWS's managed services are great for simple
            transformations, but they're not really built for that kind of
            complex stateful processing. That's where the Kubernetes Flink
            operator came in clutch. Instead of fighting with AWS's limitations,
            I could deploy Flink directly on my Kubernetes cluster and get
            exactly the stateful stream processing I needed for things like
            aggregating GPS points into routes and calculating real-time battery
            degradation patterns across ride sessions.
          </p>
          <h2 class="section-subheading">
            Cassandra - Built for Write-Heavy Workloads
          </h2>
          <p>
            I chose Apache Cassandra for storing all the aggregated trip data -
            stuff like battery stats before and after rides, calories burned,
            starting and ending GPS coordinates, basically all the summary
            information that users would want to review later. The key insight
            here was that I needed a database optimized for writes rather than
            reads. With thousands of bikes finishing trips constantly, I'd be
            writing way more data than I'd be reading, and occasional slower
            read times were totally acceptable since users aren't constantly
            refreshing their trip history. Cassandra is basically designed for
            exactly this scenario - it can handle massive write throughput
            without breaking a sweat. I briefly considered ScyllaDB, which is
            supposedly faster, but Cassandra being available as a managed
            service on AWS Keyspaces made the choice easy. Sometimes picking the
            solution with better operational support beats chasing marginal
            performance gains.
          </p>
          <h2 class="section-subheading">
            Redis - Keeping Track of the Latest Bike Data
          </h2>
          <p>
            One of the key requirements for the companion app was showing users
            their bike's current metrics in real-time - stuff like battery
            level, current speed, GPS location, you know, the basics. The tricky
            part was that some of my bikes had their own LTE connections,
            meaning they could send data directly to my servers even when the
            rider's phone wasn't connected or acting as a relay. Redis was
            perfect for this because it's basically a super-fast in-memory
            database that excels at storing and retrieving the "latest" version
            of something. Every time I got telemetry data from a bike - whether
            through MQTT from the bike's LTE connection or through the phone app
            - I'd update that bike's current state in Redis. Then when someone
            opened the app and wanted to see their current battery level or
            location, I could grab it from Redis instantly instead of having to
            dig through all the historical data in Cassandra. It's like having a
            dashboard that always shows the current readings instead of having
            to scroll through a logbook to find the most recent entry.
          </p>
          <h2 class="section-subheading">S3 - The Ultimate Backup Plan</h2>
          <p>
            AWS S3 was my insurance policy for all the raw telemetry data. While
            Redis kept the latest bike states and Cassandra stored the trip
            summaries, S3 held onto every single data point that came through my
            system - basically a complete audit trail that would let me recreate
            any trip or event from scratch if something went wrong with my other
            databases. Think of it like keeping both your important documents
            and photocopies of everything, just in case. For older data that I
            rarely accessed but needed to keep for compliance or historical
            analysis, I set up automatic archiving to S3 Glacier, which is way
            cheaper for long-term storage.
          </p>
          <h2 class="section-subheading">
            Go Microservices with PostgreSQL - Breaking Down the Features
          </h2>
          <p>
            Go was the perfect choice for building out all the different
            features the companion app needed. Instead of cramming everything
            into one massive service, I broke things down into focused
            microservices - one handling route map generation from GPS data,
            another managing fitness goals and calorie tracking, one serving up
            the latest trip data from Redis, and another handling all the safety
            features like child mode protections and geofencing alerts. For all
            the user data and app interactions - stuff like user profiles,
            fitness goal settings, bike ownership records, and safety
            preferences - I used PostgreSQL since it's rock solid for
            transactional data and complex queries. Go's lightweight nature and
            excellent concurrency support made it ideal for these services since
            they all needed to handle multiple requests simultaneously while
            doing things like processing GPS coordinates, querying user
            preferences from Postgres, or checking if a bike had wandered
            outside its allowed area. Each service could be developed, deployed,
            and scaled independently, which was huge when I needed to quickly
            push updates to the geofencing logic without touching the route
            mapping code. Plus, Go's fast startup times and small memory
            footprint meant my Kubernetes pods could spin up quickly when
            traffic spiked, keeping response times snappy even during peak
            riding hours.
          </p>
          <h2 class="section-subheading">
            Docker and Kubernetes - The Orchestration Dream Team
          </h2>
          <p>
            Docker and Kubernetes basically saved my sanity on this project.
            With all these moving pieces - multiple Go microservices, EMQX MQTT
            brokers, Flink jobs for stream processing, plus all the databases
            and caches - keeping track of everything would have been a nightmare
            without containerization. Docker let me package each service with
            exactly what it needed to run, so I never had to worry about "it
            works on my machine" problems or dependency conflicts between
            different services. Each microservice, whether it was handling route
            maps or geofencing alerts, could be containerized with its specific
            runtime requirements and deployed independently.
          </p>
          <p>
            But the real magic happened with Kubernetes orchestrating
            everything. Instead of manually deploying and babysitting dozens of
            services, Kubernetes handled service discovery, load balancing,
            health checks, and auto-scaling automatically. When traffic spiked
            during rush hours, it would spin up more instances of my route
            mapping service. When an EMQX broker crashed, it would restart it
            immediately. Even my Flink jobs for processing the telemetry streams
            ran as Kubernetes jobs, making it super easy to manage the entire
            data pipeline from one place.
          </p>
          <h2 class="section-subheading">
            Firebase - Handling Push Notifications
          </h2>
          <p>
            For push notifications, I went with Firebase Cloud Functions paired
            with Cloud Firestore to handle all the messaging logic. Whenever
            users wanted to get notified about stuff like low battery warnings,
            geofence violations, or trip summaries, I needed a reliable way to
            reach their phones. Firebase was perfect for this because it handles
            all the complexity of push notifications across different devices
            and operating systems. I stored all the FCM (Firebase Cloud
            Messaging) tokens in Cloud Firestore, which made it easy to keep
            track of which devices belonged to which users and manage token
            updates when users logged in on new phones or reinstalled the app.
            The Cloud Functions would trigger based on events from my main
            system - like when a bike's battery dropped below 20% or when
            someone's kid rode outside their allowed area - and then grab the
            right FCM tokens from Firestore to send targeted notifications. It's
            way simpler than trying to manage push notification infrastructure
            yourself, and Firebase handles all the edge cases like expired
            tokens and delivery retries automatically.
          </p>
          <h2 class="section-subheading">
            AWS Athena - Analytics and Digital Twin Features
          </h2>
          <p>
            For analytics and advanced features like my digital twin
            implementation, AWS Athena was a game-changer. All that raw
            telemetry data I was storing in S3 - every GPS coordinate, battery
            reading, speed measurement, and sensor value from thousands of bikes
            - became incredibly valuable for building insights and predictive
            features. Athena let me run SQL queries directly against the S3 data
            without having to move it anywhere or set up complex ETL pipelines.
            This was perfect for building features like digital twins, where I'd
            analyze historical patterns from a specific bike to predict things
            like battery degradation, optimal charging times, or maintenance
            needs. Instead of keeping all this analytical data in expensive
            databases, I could just point Athena at my S3 buckets and run
            queries like "show me the average battery drain rate for this bike
            model over the last six months" or "find bikes that are showing
            similar usage patterns to ones that needed maintenance." It made
            implementing sophisticated analytics features way more
            straightforward and cost-effective than traditional data warehouse
            approaches.
          </p>
          <h2 class="section-subheading">Scalability & Performance</h2>
          <div class="image-container">
            <img
              src="rendered/scalability.svg"
              alt="Emotorad Architecture Scalability"
            />
          </div>
          <ul>
            <li>
              <strong>Go Concurrency:</strong>
              Go's goroutines handled thousands of concurrent API requests and
              database queries without blocking, efficiently serving real-time
              bike data from Redis and trip histories from PostgreSQL and
              Cassandra.
            </li>
            <li>
              <strong>Independent Microservice Scaling:</strong>
              Kubernetes could scale individual services based on demand -
              spinning up more route mapping instances during rush hour without
              affecting other services.
            </li>
            <li>
              <strong>Redis for Fast Data Access:</strong>
              Redis provided instant retrieval of current bike states and user
              data, ensuring zero-delay responses in the companion app.
            </li>
            <li>
              <strong>Cassandra's Write Optimization:</strong>
              Cassandra handled massive write loads from thousands of bikes
              simultaneously without performance degradation.
            </li>
            <li>
              <strong>Kafka Message Buffering:</strong>
              Kafka acted as a shock absorber, buffering traffic spikes and
              distributing messages smoothly across the pipeline.
            </li>
            <li>
              <strong>Load Testing Validation:</strong>
              Simulated testing with 10,000+ bikes proved the architecture could
              scale well beyond current needs.
            </li>
          </ul>

          <h2 class="section-subheading">Security</h2>
          <div class="image-container">
            <img
              src="rendered/security.svg"
              alt="Emotorad Architecture Security"
            />
          </div>
          <p>
            Security was built into every layer of Emotorad's platform from the
            ground up. All data stored across my databases - PostgreSQL,
            Cassandra, Redis, and S3 - is encrypted at rest, so even if someone
            somehow got physical access to the storage, they'd just see
            gibberish. I also implemented two separate load balancers with TLS
            encryption: one specifically for EMQX to secure all the MQTT
            communication between the bikes and my servers, and another for my
            REST APIs to protect data flowing between microservices and the
            mobile app. This meant that whether a bike was sending telemetry
            data or a user was checking their trip history, everything was
            encrypted in transit.
          </p>
          <p>
            The real security win was keeping all my backend services within
            AWS's private cloud environment. Instead of services talking to each
            other over the public internet, everything stayed within my private
            network, adding an extra layer of isolation. Combined with
            Kubernetes network policies that controlled which services could
            communicate with each other, I created a pretty robust security
            perimeter. It's like having multiple locked doors between the
            outside world and your sensitive data - even if someone got through
            one layer, they'd still have several more to deal with before
            reaching anything important.
          </p>

          <h2 class="section-subheading">Deployment & Monitoring</h2>
          <div class="image-container">
            <img
              src="rendered/monitoring.svg"
              alt="Emotorad Architecture Monitoring"
            />
          </div>
          <p>
            Deployment was handled through AWS EKS (Elastic Kubernetes Service),
            which made managing my containerized microservices way less painful
            than doing it manually. EKS automatically scaled my services up and
            down based on real-time traffic - so when everyone was checking
            their bikes during morning rush hour, it would spin up more API
            instances, and then scale back down during quiet periods. This kept
            costs reasonable while ensuring users never hit slow response times
            during peak usage.
          </p>
          <p>
            For monitoring and logging, I built a custom solution using my
            existing infrastructure. All application logs flowed through Kafka
            (since I already had it handling telemetry data), and then got
            stored in MongoDB for easy querying and analysis. The cool part was
            the real-time dashboard - a WebSocket client connected directly to
            MongoDB to stream live logs, so I could watch system health and
            catch issues as they happened. This setup gave me detailed insights
            into performance bottlenecks, error patterns, and operational health
            without having to add another expensive monitoring service.
            Sometimes the best solution is just leveraging what you already have
            in a creative way.
          </p>

          <h1 class="section-heading">Implementation Details</h1>
          <h2 class="section-subheading">Challenges Faced</h2>
          <p>
            Getting efficient communication between thousands of bikes and my
            backend was trickier than expected. JSON was way too verbose for the
            constant stream of telemetry data I was dealing with, so I switched
            to Protocol Buffers for data serialization. Protobuf's compact
            binary format significantly reduced bandwidth usage - crucial when
            bikes were on cellular connections with limited data plans. The
            schema definition also gave me strong typing and backward
            compatibility, so I could evolve my data structures without breaking
            existing bikes in the field. Designing the protobuf schemas required
            balancing between keeping messages small while ensuring I captured
            all the necessary telemetry data like GPS coordinates, battery
            levels, and sensor readings.
          </p>
          <p>
            Managing stateful services on Kubernetes was another headache,
            especially for my EMQX brokers and databases that needed to maintain
            state across restarts. I had to implement StatefulSets with
            persistent volume claims to ensure that when pods crashed or got
            rescheduled, they could pick up exactly where they left off. This
            was particularly important for services that held critical state
            like MQTT broker subscriptions or cached data. Getting the
            persistent storage configuration right took some experimentation,
            but eventually I had a setup where services could survive pod
            failures and cluster maintenance without losing important state or
            causing service disruptions.
          </p>
          <p>
            Ensuring proper checkpointing and recovery in Apache Flink was
            crucial for maintaining data integrity in my stream processing jobs.
            Flink jobs were calculating real-time analytics like trip summaries
            and battery usage patterns, so losing state would mean losing track
            of ongoing calculations. I implemented checkpointing mechanisms that
            periodically saved the state of Flink jobs to persistent storage,
            allowing them to recover and continue from the last successful
            checkpoint after failures. Getting the checkpointing intervals and
            recovery strategies right required fine-tuning based on my specific
            workload patterns - too frequent and it would impact performance,
            too infrequent and I'd risk losing too much progress during
            failures.
          </p>

          <h2 class="section-subheading">Key Features</h2>
          <ul>
            <li>
              <strong>Efficient Data Serialization:</strong>
              Used Protocol Buffers (protobuf) for compact, efficient data
              exchange between mobile devices and the backend.
            </li>
            <li>
              <strong>Stateful Service Management:</strong>
              Leveraged Kubernetes StatefulSets and persistent volumes for data
              consistency and high availability.
            </li>
            <li>
              <strong>Advanced Checkpointing:</strong>
              Configured Flink checkpointing to ensure data recovery and
              integrity during failures.
            </li>
            <li>
              <strong>Real-Time Data Processing:</strong>
              Developed stateful Flink applications for real-time data
              processing with Cassandra, Redis, and Kafka.
            </li>
            <li>
              <strong>Microservice Architecture:</strong>
              Built and maintained Go REST microservices for dashboards and the
              Emotorad companion app.
            </li>
            <li>
              <strong>Dynamic Deployment and Scaling:</strong>
              Used AWS EKS and Kubernetes with HPA and Flink Kubernetes operator
              for scalable deployments.
            </li>
            <li>
              <strong>Logging and Monitoring:</strong>
              Implemented logging with MongoDB and Kafka, and real-time log
              display with a WebSocket client.
            </li>
            <li>
              <strong>Secure Encryption:</strong>
              Applied TLS encryption for MQTT and REST APIs, and encryption at
              rest for stateful services.
            </li>
          </ul>

          <h1 class="section-heading">Testing & Deployment</h1>
          <h2 class="section-subheading">Testing Strategies</h2>
          <p>
            For testing APIs, I implemented mocking through dependency
            injection, which allowed for isolating the APIs from external
            services. By injecting mocked dependencies during testing, I could
            simulate various scenarios, ensuring that the APIs were thoroughly
            tested without relying on external systems. This approach provided a
            controlled testing environment, resulting in more reliable and
            efficient tests.
          </p>
          <p>
            I developed tests for database queries using sqlc in Go. These tests
            were designed to validate the correctness of SQL queries generated
            by sqlc, ensuring accurate data retrieval and manipulation. By
            focusing on the integrity of the database interactions.
          </p>

          <h2 class="section-subheading">Deployment Automation</h2>
          <div class="image-container">
            <img
              src="rendered/deployment.svg"
              alt="Emotorad Deployment Automation"
            />
          </div>
          <p>
            Initially, I used a Makefile to automate the deployment process for
            Emotorad. The Makefile pushed Docker images to AWS Elastic Container
            Registry (ECR) and then employed kubectl commands to pull the latest
            image and restart the deployment on Kubernetes. To improve
            efficiency and streamline automation, I transitioned to GitHub
            Actions, which integrates deployment tasks directly with the
            repository. Future plans include adopting Jenkins to further enhance
            and automate the deployment pipeline.
          </p>
        </div>

        <div class="right-sidebar">
          <h1>Tools Used</h1>

          <h2 class="section-heading">Languages</h2>
          <a href="https://go.dev/" target="_blank">Go</a>
          <a href="https://www.java.com/en/" target="_blank">Java</a>
          <a href="https://nodejs.org/en" target="_blank">Node.js</a>
          <a href="https://www.typescriptlang.org/" target="_blank">
            TypeScript
          </a>

          <h2 class="section-heading">Stateful Services</h2>
          <a href="https://www.postgresql.org/" target="_blank">PostgreSQL</a>
          <a href="https://cassandra.apache.org/_/index.html" target="_blank">
            Apache Cassandra
          </a>
          <a href="https://redis.io/" target="_blank">Redis</a>
          <a href="https://firebase.google.com/docs/firestore" target="_blank">
            Cloud Firestore
          </a>
          <a href="https://www.mongodb.com/" target="_blank">MongoDB</a>
          <a href="https://aws.amazon.com/s3/" target="_blank">AWS S3</a>

          <h2 class="section-heading">Notifications</h2>
          <a
            href="https://firebase.google.com/docs/cloud-messaging"
            target="_blank"
          >
            Firebase Cloud Messaging
          </a>
          <a href="https://firebase.google.com/docs/functions" target="_blank">
            Firebase Cloud Functions
          </a>
          <a href="https://firebase.google.com/docs/firestore" target="_blank">
            Cloud Firestore
          </a>

          <h2 class="section-heading">Streaming Services & Formats</h2>
          <a href="https://kafka.apache.org/" target="_blank">Apache Kafka</a>
          <a href="https://flink.apache.org/" target="_blank">Apache Flink</a>
          <a href="https://www.emqx.com/en" target="_blank">EMQX MQTT</a>
          <a href="https://parquet.apache.org/" target="_blank">
            Apache Parquet
          </a>
          <a href="https://protobuf.dev/" target="_blank">Protocol Buffers</a>
          <a href="https://www.json.org/json-en.html" target="_blank">JSON</a>

          <h2 class="section-heading">Analytics</h2>
          <a href="https://trino.io/" target="_blank">Trino</a>

          <h2 class="section-heading">Deployment</h2>
          <a href="https://kubernetes.io/" target="_blank">Kubernetes</a>
          <a href="https://www.docker.com/" target="_blank">Docker</a>

          <h2 class="section-heading">Amazon Web Services</h2>
          <a href="https://aws.amazon.com/athena/" target="_blank">Athena</a>
          <a href="https://aws.amazon.com/ecs/" target="_blank">
            Elastic Container Service
          </a>
          <a href="https://aws.amazon.com/ecr/" target="_blank">
            Elastic Container Registry
          </a>
          <a href="https://aws.amazon.com/eks/" target="_blank">
            Elastic Kubernetes Service
          </a>
          <a
            href="https://aws.amazon.com/elasticloadbalancing/"
            target="_blank"
          >
            Elastic Load Balancing
          </a>
          <a href="https://aws.amazon.com/ec2/" target="_blank">
            Elastic Cloud Compute
          </a>
          <a href="https://aws.amazon.com/keyspaces/" target="_blank">
            Keyspaces
          </a>
          <a href="https://aws.amazon.com/memorydb/" target="_blank">
            MemoryDB
          </a>
          <a href="https://aws.amazon.com/msk/" target="_blank">
            Managed Streaming for Apache Kafka
          </a>
          <a href="https://aws.amazon.com/rds/" target="_blank">
            Relational Database Service
          </a>
          <a href="https://aws.amazon.com/s3/" target="_blank">
            Simple Storage Service
          </a>
        </div>
      </div>
    </div>
  </body>
</html>
